Target audience: Readers of a published technical book who want accurate, context-aware answers strictly derived from the bookâ€™s content. Focus: - AI-native Retrieval-Augmented Generation (RAG) - Embedded chatbot inside a book website - Strict context enforcement with zero hallucination - Dual interaction modes: full-book and selection-only Success criteria: - Chatbot responses are generated only from book content - Selection-only mode never performs vector retrieval - System explicitly refuses when context is insufficient - Answers reference chapter/section metadata when available - All RAG steps are traceable and reviewable Constraints: - Backend framework: FastAPI (Python) - LLM provider: Cohere API only - Vector database: Qdrant Cloud (Free Tier) - Relational database: Neon Serverless Postgres - Development workflow: SpecKit + Gemini CLI - No hardcoded secrets allowed in prompts or code RAG behavior requirements: - Book-wide mode: - Query Qdrant using Cohere embeddings - Retrieve top-k relevant chunks - Inject retrieved context into generation prompt - Selection-only mode: - Disable vector search entirely - Use only user-selected text as context - Refuse politely if question exceeds provided text Verification requirements: - Every response must be grounded in provided context - No external or world knowledge allowed - All retrieval and validation steps must be logged - Errors and refusals must be user-readable and explicit Not building: - Open-domain or internet-connected chatbot - Model fine-tuning or training - Content generation for the book itself - Recommendation or personalization system - Analytics or monetization features Timeline: - Phase 1: Technical specification - Phase 2: Implementation plan - Phase 3: Production-grade code - Each phase must be completed and reviewed before proceeding